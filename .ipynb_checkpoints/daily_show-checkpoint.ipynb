{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/Cellar/apache-spark/2.2.0/libexec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We automatically have access to the SparkContext object sc. We then run the following code to read the TSV data set into an RDD object raw_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"daily_show.tsv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD object raw_data closely resembles a list of string objects, with one object for each line in the data set. We then use the take() method to print the first five elements of the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR\\tGoogleKnowlege_Occupation\\tShow\\tGroup\\tRaw_Guest_List',\n",
       " '1999\\tactor\\t1/11/99\\tActing\\tMichael J. Fox',\n",
       " '1999\\tComedian\\t1/12/99\\tComedy\\tSandra Bernhard',\n",
       " '1999\\ttelevision actress\\t1/13/99\\tActing\\tTracey Ullman',\n",
       " '1999\\tfilm actress\\t1/14/99\\tActing\\tGillian Anderson']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark offers many advantages over regular Python, though. For example, thanks to RDD abstraction, you can run Spark locally on your own computer. Spark will simulate distributing your calculations over many machines by automatically slicing your computer's memory into partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's RDD implementation also lets us evaluate code \"lazily,\" meaning we can postpone running a calculation until absolutely necessary. On the previous screen, Spark waited to load the TSV file into an RDD until raw_data.take(5) executed. When our code called raw_data = sc.textFile(\"dail_show.tsv\"), Spark created a pointer to the file, but didn't actually read it into raw_data until raw_data.take(5) needed that variable to run its logic.\n",
    "\n",
    "The advantage of \"lazy\" evaluation is that we can build up a queue of tasks and let Spark optimize the overall workflow in the background. In regular Python, the interpreter can't do much workflow optimization. We'll see more examples of lazy evaluation later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea to understand when working with Spark is data pipelining. Every operation or calculation in Spark is essentially a series of steps that we can chain together and run in succession to form a pipeline. Each step in the pipeline returns either a Python value (such as an integer), a Python data structure (such as a dictionary), or an RDD object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map()\n",
    "\n",
    "The map(f) function applies the function f to every element in the RDD. Because RDDs are iterable objects (like most Python objects), Spark runs function f on each iteration and returns a new RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look carefully, you'll see that raw_data is in a format that's hard to work with. While the elements are currently all strings, we'd like to convert each of them into a list to make the data more manageable. To do this the traditional way, we would:\n",
    "\n",
    "1. Use a 'for' loop to iterate over the collection\n",
    "2. Split each `string` on the delimiter\n",
    "3. Store the result in a `list`\n",
    "\n",
    "Let's see how we can use map to do this with Spark instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'],\n",
       " ['1999', 'actor', '1/11/99', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '1/12/99', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "daily_show.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the function was in Python, we also took advantage of Scala when Spark actually ran the code over our RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without learning any Scala, we get to harness the data processing performance gains from Spark's Scala architecture. Even better, when we ran:\n",
    "\n",
    "```\n",
    "daily_show.take(5)\n",
    "```\n",
    "\n",
    "it returned the results to us in Python-friendly notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "There are two types of methods in Spark:\n",
    "\n",
    "1. Transformations - map(), reduceByKey()\n",
    "2. Actions - take(), reduce(), saveAsTextFile(), collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are lazy operations that always return a reference to an RDD object. Spark doesn't actually run the transformations, though, until an action needs to use the RDD resulting from a transformation. Any function that returns an RDD is a transformation, and any function that returns a value is an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immutability\n",
    "\n",
    "\n",
    "RDD objects are immutable, meaning that we can't change their values once we've created them.\n",
    "\n",
    "In Python, list and dictionary objects are mutable (we can change their values), while tuple objects are immutable. The only way to modify a tuple object in Python is to create a new tuple object with the necessary updates. \n",
    "\n",
    "Spark uses the immutability of RDDs to enhance calculation speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to tally up the number of guests who have appeared on The Daily Show during each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we do this in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys in tally will be the years, and the values will be the totals for the number of lines associated with each year.\n",
    "\n",
    "To achieve the same result with Spark, we'll have to use a Map step, then a ReduceByKey step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1), ('2002', 159), ('2003', 166), ('2004', 164), ('2007', 141)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "tally.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the map step, we used a lambda function to create a tuple consisting of:\n",
    "\n",
    "key: x[0] (the first value in the list)\n",
    "value: 1 (the integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our high-level strategy was to create a tuple with the key representing the year, and the value representing 1. After running the map step, Spark will maintain in memory a list of tuples resembling the following:\n",
    "\n",
    "```\n",
    "('YEAR', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "...\n",
    "```\n",
    "\n",
    "We'd like to reduce that down to:\n",
    "\n",
    "```\n",
    "('YEAR', 1)\n",
    "('1991', 4)\n",
    "...\n",
    "```\n",
    "\n",
    "reduceByKey(f) combines tuples with the same key using the function we specify, f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1),\n",
       " ('2002', 159),\n",
       " ('2003', 166),\n",
       " ('2004', 164),\n",
       " ('2007', 141),\n",
       " ('2010', 165),\n",
       " ('2011', 163),\n",
       " ('2012', 164),\n",
       " ('2013', 166),\n",
       " ('2014', 163),\n",
       " ('2015', 100),\n",
       " ('1999', 166),\n",
       " ('2000', 169),\n",
       " ('2001', 157),\n",
       " ('2005', 162),\n",
       " ('2006', 161),\n",
       " ('2008', 164),\n",
       " ('2009', 163)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tally.take(tally.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to remove the element ('YEAR', 1) from our collection. We'll need a workaround, though, because RDD objects are immutable once we create them. The only way to remove that tuple is to create a new RDD object that doesn't have it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark comes with a filter(f) function that creates a new RDD by filtering an existing one for specific criteria. If we specify a function f that returns a binary value, True or False, the resulting RDD will consist of elements where the function evaluated to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_year(line):\n",
    "    # Write your logic here\n",
    "    if line[0] == 'YEAR':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_daily_show = daily_show.filter(lambda line: filter_year(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1999', 'actor', '1/11/99', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '1/12/99', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson'],\n",
       " ['1999', 'actor', '1/18/99', 'Acting', 'David Alan Grier']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_daily_show.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flex Spark's muscles, we'll demonstrate how to chain together a series of data transformations into a pipeline, and observe Spark managing everything in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Spark came along, running lots of tasks in succession in Hadoop was incredibly time consuming. Hadoop had to write intermediate results to disk, and wasn't aware of the full pipeline. Thanks to its aggressive approach to memory use and well-architected core, Spark improves on Hadoop's turnaround time significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell, we'll filter out actors for whom the profession is blank, lowercase each profession, generate a count of professions, and output the first five tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 596),\n",
       " ('film actress', 21),\n",
       " ('model', 9),\n",
       " ('stand-up comedian', 44),\n",
       " ('actress', 271)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_daily_show.filter(lambda line: line[1] != '') \\\n",
    "                   .map(lambda line: (line[1].lower(), 1)) \\\n",
    "                   .reduceByKey(lambda x,y: x+y) \\\n",
    "                   .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
